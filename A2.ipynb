{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ9OL7QjDAuV",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Обучение линейного классификатора \n",
    "\n",
    "Для выполнения этого задания нужно будет дописать код в этом ноутбуке  \n",
    "\n",
    "В этом упражнении Вам предстоит:\n",
    "\n",
    "- реализовать функцию потерь **CrossEntropyLoss** с регуляризацией для батча произвольного размера\n",
    "- реализовать векторизованную функцию для вычисления **аналитического градиента**\n",
    "- **проверить свою реализацию** с градиентом, вычисленным методом конечных разностей\n",
    "- **оптимизировать** веса линейного классификатора с помощью градиентного спуска **SGD**\n",
    "- **найти лучшие learning rate и regularization** \n",
    "- **визуализировать** матрицу оптимальных весов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scj2cxlwDAua",
    "outputId": "4f3ab73c-703b-4924-808d-be5a1b0045c2"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "%%bash\n",
    "git clone https://github.com/balezz/modern_dl.git\n",
    "cd modern_dl\n",
    "mkdir data\n",
    "cd data\n",
    "wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
    "tar -xzvf cifar-10-python.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfE_l7wlDNzd",
    "outputId": "47b0efe1-07fe-44be-e2c2-896866653219"
   },
   "outputs": [],
   "source": [
    "%cd modern_dl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZFniEy8DAuc",
    "outputId": "89695f38-9e56-446e-dc5d-808640d924fd",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from lib.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.gradient_check import check_gradient\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI6Y5fHiJx9e"
   },
   "source": [
    "# Загрузка датасета CIFAR-10 и предварительная подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "065Yd2AjDAud",
    "outputId": "8b3737c5-0d01-4d06-b0e9-8088ce314361",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Путь к папке с данными\n",
    "cifar10_dir = 'data/cifar-10-batches-py'\n",
    "\n",
    "# Очистим значения переменных, чтобы избежать проблем с излишним потреблением памяти\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Проверим размер входных и выходных векторов.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "xLmw-REjJ8Do",
    "outputId": "bd4860f1-4b73-4867-8966-841730d8f494",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Перед началом работы полезно посмотреть на данные.\n",
    "# Отобразим пример из каждого класса.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1MTg2q5KDZw",
    "outputId": "108e9182-b302-4c2a-f4f7-decc82281b40"
   },
   "outputs": [],
   "source": [
    "# Для удобства преобразуем двумерные изображения в одномерные вектора\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Проверим размер полученных данных\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Test data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "I5NPvm_fKG9o",
    "outputId": "86c49c2b-3cbc-4978-ffb8-54044def164e"
   },
   "outputs": [],
   "source": [
    "# Нормализуем значения яркости пикселей \n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print(mean_image[:10]) \n",
    "\n",
    "# визуализируем среднюю яркость\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "plt.show()\n",
    "\n",
    "# Вычтем средние значения яркости\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# Добавим к вектору исходных данных фиктивный признак с постоянным значением 1.\n",
    "# Этот трюк позволит избежать лишних вычислений: x @ W + b  => x' @ W'\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOvIlepPDAug",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Softmax Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X7RgnojHLLe"
   },
   "outputs": [],
   "source": [
    "p = np.arange(40).reshape(4, 10)\n",
    "y = np.arange(4)\n",
    "\n",
    "print('p = \\n', p)\n",
    "print('y = \\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[range(4), y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# Реализуйте эффективную векторизованную функцию вычисления CELoss и dW\n",
    "\n",
    "def softmax_loss(W, X, y, reg=1e-5):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    \n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    # print(loss)\n",
    "    return loss, dL_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hbd-SlbFDAuh",
    "outputId": "76ce93ea-ff62-49c1-b8f4-be54dcd51166",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Инициализируем веса значениями близкими, но не равными нулю\n",
    "W = np.random.randn(3073, 10) * 1e-5\n",
    "loss, grad = softmax_loss(W, X_test, y_test, 0.0)\n",
    "\n",
    "# Проверим правильность реализации\n",
    "# Для 10 классов loss должен быть около -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('Начальное значение Loss = %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим градиент с помощью метода конечных разностей\n",
    "check_gradient(softmax_loss, X_test[:1], y_test[:1], W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 1**  Почему значения, вычисленные аналитическим и численным методом, отличаются после 7 знака? Какое значение более правильное, по Вашему мнению?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94MroGpUK1_t"
   },
   "source": [
    "# Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\frac{dL}{dz} = [ \\begin{array}{cc} \n",
    "\\frac{dL}{dz_1}\\\\\n",
    "\\frac{dL}{dz_2}\n",
    "\\end{array}  ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\frac{dL}{dx} =  \\begin{array}{cc} \n",
    "\\frac{dL}{dx_1}\\\\\n",
    "\\frac{dL}{dx_2} \\\\\n",
    "\\frac{dL}{dx_3} \n",
    "\\end{array}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large X @ W =  \\begin{array}{cc} \n",
    "w_{0,0} \\cdot x_0 + w_{0,1} \\cdot x_1 + w_{0,2} \\cdot x_2 \\\\\n",
    "w_{1,0} \\cdot x_0 + w_{1,1} \\cdot x_1 + w_{1,2} \\cdot x_2 \\\\\n",
    "\\end{array} = \n",
    "\\begin{array}{cc} \n",
    "z_0 \\\\\n",
    "z_1\n",
    "\\end{array} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\frac{dz}{dx} =  \\begin{array}{cc} \n",
    "\\frac{dz_0}{dx_0}, \\frac{dz_0}{dx_1}, \\frac{dz_0}{dx_2} \\\\\n",
    "\\frac{dz_1}{dx_0}, \\frac{dz_1}{dx_1}, \\frac{dz_1}{dx_2}\n",
    "\\end{array}  = W $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\frac{dL}{dW} = [ \\begin{array}{cc} \n",
    "\\frac{dL}{dW_{0,0} }, \\frac{dL}{dW_{0,1}}, \\frac{dL}{dW_{0,2}} \\\\\n",
    "\\frac{dL}{dW_{1,0} }, \\frac{dL}{dW_{1,1}}, \\frac{dL}{dW_{1,2}}\n",
    "\\end{array}  ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yF-MdGCK8lZ"
   },
   "outputs": [],
   "source": [
    "# Реализуйте методы predict и train с оптимизацией SGD  \n",
    "\n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate, reg, num_iters=1000,\n",
    "              batch_size=64, verbose=True):\n",
    "        \"\"\"\n",
    "        Обучение классификатора с помощью стохастического градиентного спуска\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "          means that X[i] has label 0 <= c < C for C classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # lazily initialize W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Sample batch_size elements from the training data and their           #\n",
    "            # corresponding labels to use in this round of gradient descent.        #\n",
    "            # Store the data in X_batch and their corresponding labels in           #\n",
    "            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
    "            # and y_batch should have shape (batch_size,)                           #\n",
    "            #                                                                       #\n",
    "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "            # replacement is faster than sampling without replacement.              #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            self.W -= grad*learning_rate\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f '  % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted labels in y_pred.            #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative.\n",
    "        Subclasses will override this.\n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "          data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength.\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        return softmax_loss(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RkYWqdb2K1St",
    "outputId": "b5853f96-6c28-4592-8fc3-774411fc84a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "softmax_cls = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = softmax_cls.train(X_test, y_test, learning_rate=1e-5, reg=1e-5,\n",
    "                      num_iters=10000, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "nIAQM6c-MkoH",
    "outputId": "3d672585-ccd1-40bb-ecde-4212860dbd6b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Построим график зависимости loss от количества итераций\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxoJZ186Mvgv",
    "outputId": "dba8fce8-42e8-426b-ad59-d25975bea507",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# оцените точность предсказания на выборках train и val\n",
    "y_train_pred = softmax_cls.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_test_pred = softmax_cls.predict(X_test)\n",
    "print('test accuracy: %f' % (np.mean(y_test == y_test_pred), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKwv4sTdNEvU"
   },
   "source": [
    "# Поиск лучших гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLB88C5BM-_D",
    "outputId": "a7dfdb33-3391-46d1-aacc-ed1beda380f9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Используйте валидационную выборку для выбора лучших гиперпараметров \n",
    "# (learning rate and regularization strength)\n",
    "# Добейтесь точности не меньше 0.35 на выборке test.\n",
    "# Используйте словарь results в котором \n",
    "# ключи - кортеж  (learning_rate, regularization_strength)\n",
    "# значения - (train_accuracy, test_accuracy)\n",
    "# Точность вычисляется как отношение числа верно предсказанных классов \n",
    "# к объему выборки\n",
    "results = {}\n",
    "best_test = -1   \n",
    "best_cls = None # Лучший экземпляр Softmax classifier \n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Напишите код, позволяющий найти лучшее значение гиперпараметров на test      #\n",
    "# выборке. Для каждой комбинации гиперпараметров обучите классификатор         #\n",
    "# на train выборке, вычислите точность на выборках train, test и сохраните     #\n",
    "# результат в словарь results. Лучшее значение точности сохраните в best_test  #\n",
    "# лучший классификатор - в best_svm                                            #\n",
    "################################################################################\n",
    "\n",
    "# Пример допустимых значений. Можете изменить на свое усмотрение.\n",
    "learning_rates = [1e-6, 1e-5, 1e-4]\n",
    "regularization_strengths = [10**n for n in range(-8, 1)]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Вывод результатов.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, test_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f test accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "D9amiJoBNDIC",
    "outputId": "6f86e6ee-2b88-4ae9-c755-8e5369a4bff3"
   },
   "outputs": [],
   "source": [
    "# Визуализируем результаты \n",
    "import math\n",
    "import pdb\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# график accuracy на обучении\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# график accuracy на test выборке\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghx3RGPPDAuk",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "# Точность на test выборке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "IFONVgKYr5bS",
    "outputId": "9e4bfead-0330-4f11-e5e1-009fabb5dcd3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = best_cls.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_cls.train(X_train, y_train, learning_rate=1e-7, reg=1e-7,\n",
    "                          num_iters=100000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap4ZunISDAul"
   },
   "outputs": [],
   "source": [
    "# Визуализируем веса W для каждого класса\n",
    "plt.rcParams['image.interpolation'] = 'quadric'\n",
    "w = best_cls.W[:-1,:] # отбросим фиктивное измерение bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "print(w_min, w_max)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Масштабируем веса в значения от 0 до 255 для визуализации\n",
    "    wimg = 255.0 * (w[:, :, :, i] - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2-Softmax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
